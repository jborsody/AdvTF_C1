{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svyum7G3gHcK"
      },
      "source": [
        "## Ungraded Lab: Convolutional Autoencoders\n",
        "\n",
        "In this lab, you will use convolution layers to build your autoencoder. This usually leads to better results than dense networks and you will see it in action with the [Fashion MNIST dataset](https://www.tensorflow.org/datasets/catalog/fashion_mnist)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk0Tld-U5XFD"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3EXwoz-KHtWO"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "  \n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0WGuXlw5bK-"
      },
      "source": [
        "## Prepare the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTySDKEhLNLY"
      },
      "source": [
        "As before, you will load the train and test sets from TFDS. Notice that we don't flatten the image this time. That's because we will be using convolutional layers later that can deal with 2D images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t9F7YsCNIKSA"
      },
      "outputs": [],
      "source": [
        "def map_image(image, label):\n",
        "  '''Normalizes the image. Returns image as input and label.'''\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = image / 255.0\n",
        "\n",
        "  return image, image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9ZsciqJXL368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M1 Pro\n",
            "\n",
            "systemMemory: 32.00 GB\n",
            "maxCacheSize: 10.67 GB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 128\n",
        "SHUFFLE_BUFFER_SIZE = 1024\n",
        "\n",
        "train_dataset = tfds.load('fashion_mnist', as_supervised=True, split=\"train\")\n",
        "train_dataset = train_dataset.map(map_image)\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "test_dataset = tfds.load('fashion_mnist', as_supervised=True, split=\"test\")\n",
        "test_dataset = test_dataset.map(map_image)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).repeat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoyz09uKMDn5"
      },
      "source": [
        "## Define the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1-Fw_qnZPV7"
      },
      "source": [
        "As mentioned, you will use convolutional layers to build the model. This is composed of three main parts: encoder, bottleneck, and decoder. You will follow the configuration shown in the image below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "568W0TYyY9nl"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=15zh7bst9KKvciRdCvMAH7kXt3nNkABzO\" width=\"75%\" height=\"75%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2IvtyIoZnb4"
      },
      "source": [
        "The encoder, just like in previous labs, will contract with each additional layer. The features are generated with the Conv2D layers while the max pooling layers reduce the dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wxh8h-UMk2iL"
      },
      "outputs": [],
      "source": [
        "def encoder(inputs):\n",
        "  '''Defines the encoder with two Conv2D and max pooling layers.'''\n",
        "  conv_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
        "  max_pool_1 = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(conv_1)\n",
        "\n",
        "  conv_2 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(max_pool_1)\n",
        "  max_pool_2 = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(conv_2)\n",
        "\n",
        "  return max_pool_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9KQYnabazLl"
      },
      "source": [
        "A bottleneck layer is used to get more features but without further reducing the dimension afterwards. Another layer is inserted here for visualizing the encoder output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wRWmLA3VliDr"
      },
      "outputs": [],
      "source": [
        "def bottle_neck(inputs):\n",
        "  '''Defines the bottleneck.'''\n",
        "  bottle_neck = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
        "  encoder_visualization = tf.keras.layers.Conv2D(filters=1, kernel_size=(3,3), activation='sigmoid', padding='same')(bottle_neck)\n",
        "\n",
        "  return bottle_neck, encoder_visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FayvcE3ebZxk"
      },
      "source": [
        "The decoder will upsample the bottleneck output back to the original image size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XZgLt5uAmArk"
      },
      "outputs": [],
      "source": [
        "def decoder(inputs):\n",
        "  '''Defines the decoder path to upsample back to the original image size.'''\n",
        "  conv_1 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
        "  up_sample_1 = tf.keras.layers.UpSampling2D(size=(2,2))(conv_1)\n",
        "\n",
        "  conv_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(up_sample_1)\n",
        "  up_sample_2 = tf.keras.layers.UpSampling2D(size=(2,2))(conv_2)\n",
        "\n",
        "  conv_3 = tf.keras.layers.Conv2D(filters=1, kernel_size=(3,3), activation='sigmoid', padding='same')(up_sample_2)\n",
        "\n",
        "  return conv_3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvfhvk9qbvCp"
      },
      "source": [
        "You can now build the full autoencoder using the functions above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fQKwO64iiOYl"
      },
      "outputs": [],
      "source": [
        "def convolutional_auto_encoder():\n",
        "  '''Builds the entire autoencoder model.'''\n",
        "  inputs = tf.keras.layers.Input(shape=(28, 28, 1,))\n",
        "  encoder_output = encoder(inputs)\n",
        "  bottleneck_output, encoder_visualization = bottle_neck(encoder_output)\n",
        "  decoder_output = decoder(bottleneck_output)\n",
        "  \n",
        "  model = tf.keras.Model(inputs =inputs, outputs=decoder_output)\n",
        "  encoder_model = tf.keras.Model(inputs=inputs, outputs=encoder_visualization)\n",
        "  return model, encoder_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1MmS7r0tkuIf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 28, 28, 64)        640       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 256)         295168    \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 7, 7, 128)         295040    \n",
            "                                                                 \n",
            " up_sampling2d (UpSampling2D  (None, 14, 14, 128)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 14, 14, 64)        73792     \n",
            "                                                                 \n",
            " up_sampling2d_1 (UpSampling  (None, 28, 28, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 28, 28, 1)         577       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 739,073\n",
            "Trainable params: 739,073\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "convolutional_model, convolutional_encoder_model = convolutional_auto_encoder()\n",
        "convolutional_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FRxRr0LMLCs"
      },
      "source": [
        "## Compile and Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "J0Umj_xaiHL_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-07 12:59:07.826266: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "468/468 [==============================] - 33s 63ms/step - loss: 0.2847 - val_loss: 0.2662\n",
            "Epoch 2/40\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 0.2595 - val_loss: 0.2581\n",
            "Epoch 3/40\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 0.2547 - val_loss: 0.2552\n",
            "Epoch 4/40\n",
            "468/468 [==============================] - 29s 63ms/step - loss: 0.2523 - val_loss: 0.2538\n",
            "Epoch 5/40\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 0.2509 - val_loss: 0.2522\n",
            "Epoch 6/40\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 0.2500 - val_loss: 0.2517\n",
            "Epoch 7/40\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 0.2493 - val_loss: 0.2508\n",
            "Epoch 8/40\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 0.2487 - val_loss: 0.2503\n",
            "Epoch 9/40\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 0.2482 - val_loss: 0.2503\n",
            "Epoch 10/40\n",
            "468/468 [==============================] - 29s 63ms/step - loss: 0.2478 - val_loss: 0.2496\n",
            "Epoch 11/40\n",
            "468/468 [==============================] - 30s 64ms/step - loss: 0.2476 - val_loss: 0.2495\n",
            "Epoch 12/40\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 0.2472 - val_loss: 0.2490\n",
            "Epoch 13/40\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.2468 - val_loss: 0.2487\n",
            "Epoch 14/40\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 0.2467 - val_loss: 0.2496\n",
            "Epoch 15/40\n",
            "468/468 [==============================] - 30s 63ms/step - loss: 0.2464 - val_loss: 0.2483\n",
            "Epoch 16/40\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 0.2463 - val_loss: 0.2481\n",
            "Epoch 17/40\n",
            "468/468 [==============================] - 29s 63ms/step - loss: 0.2461 - val_loss: 0.2481\n",
            "Epoch 18/40\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 0.2458 - val_loss: 0.2480\n",
            "Epoch 19/40\n",
            "468/468 [==============================] - 29s 63ms/step - loss: 0.2458 - val_loss: 0.2479\n",
            "Epoch 20/40\n",
            "468/468 [==============================] - 30s 63ms/step - loss: 0.2457 - val_loss: 0.2478\n",
            "Epoch 21/40\n",
            "468/468 [==============================] - 29s 63ms/step - loss: 0.2454 - val_loss: 0.2475\n",
            "Epoch 22/40\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 0.2455 - val_loss: 0.2473\n",
            "Epoch 23/40\n",
            "468/468 [==============================] - 30s 65ms/step - loss: 0.2451 - val_loss: 0.2472\n",
            "Epoch 24/40\n",
            "468/468 [==============================] - 30s 64ms/step - loss: 0.2452 - val_loss: 0.2471\n",
            "Epoch 25/40\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.2450 - val_loss: 0.2470\n",
            "Epoch 26/40\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.2449 - val_loss: 0.2469\n",
            "Epoch 27/40\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.2449 - val_loss: 0.2469\n",
            "Epoch 28/40\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.2448 - val_loss: 0.2469\n",
            "Epoch 29/40\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 0.2447 - val_loss: 0.2468\n",
            "Epoch 30/40\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 0.2447 - val_loss: 0.2473\n",
            "Epoch 31/40\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.2445 - val_loss: 0.2466\n",
            "Epoch 32/40\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.2444 - val_loss: 0.2466\n",
            "Epoch 33/40\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.2445 - val_loss: 0.2466\n",
            "Epoch 34/40\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.2444 - val_loss: 0.2467\n",
            "Epoch 35/40\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.2443 - val_loss: 0.2464\n",
            "Epoch 36/40\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.2442 - val_loss: 0.2464\n",
            "Epoch 37/40\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.2443 - val_loss: 0.2463\n",
            "Epoch 38/40\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.2441 - val_loss: 0.2463\n",
            "Epoch 39/40\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.2441 - val_loss: 0.2462\n",
            "Epoch 40/40\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.2442 - val_loss: 0.2461\n"
          ]
        }
      ],
      "source": [
        "train_steps = 60000 // BATCH_SIZE\n",
        "valid_steps = 60000 // BATCH_SIZE\n",
        "\n",
        "convolutional_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(), loss='binary_crossentropy')\n",
        "conv_model_history = convolutional_model.fit(train_dataset, steps_per_epoch=train_steps, validation_data=test_dataset, validation_steps=valid_steps, epochs=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8zE9OiAMUd7"
      },
      "source": [
        "## Display sample results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCUOM7F_cf26"
      },
      "source": [
        "As usual, let's see some sample results from the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A35RlIqKIsQv"
      },
      "outputs": [],
      "source": [
        "def display_one_row(disp_images, offset, shape=(28, 28)):\n",
        "  '''Display sample outputs in one row.'''\n",
        "  for idx, test_image in enumerate(disp_images):\n",
        "    plt.subplot(3, 10, offset + idx + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    test_image = np.reshape(test_image, shape)\n",
        "    plt.imshow(test_image, cmap='gray')\n",
        "\n",
        "\n",
        "def display_results(disp_input_images, disp_encoded, disp_predicted, enc_shape=(8,4)):\n",
        "  '''Displays the input, encoded, and decoded output values.'''\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  display_one_row(disp_input_images, 0, shape=(28,28,))\n",
        "  display_one_row(disp_encoded, 10, shape=enc_shape)\n",
        "  display_one_row(disp_predicted, 20, shape=(28,28,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtQyQRxRN_hH"
      },
      "outputs": [],
      "source": [
        "# take 1 batch of the dataset\n",
        "test_dataset = test_dataset.take(1)\n",
        "\n",
        "# take the input images and put them in a list\n",
        "output_samples = []\n",
        "for input_image, image in tfds.as_numpy(test_dataset):\n",
        "      output_samples = input_image\n",
        "\n",
        "# pick 10 indices\n",
        "idxs = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "\n",
        "# prepare test samples as a batch of 10 images\n",
        "conv_output_samples = np.array(output_samples[idxs])\n",
        "conv_output_samples = np.reshape(conv_output_samples, (10, 28, 28, 1))\n",
        "\n",
        "# get the encoder ouput\n",
        "encoded = convolutional_encoder_model.predict(conv_output_samples)\n",
        "\n",
        "# get a prediction for some values in the dataset\n",
        "predicted = convolutional_model.predict(conv_output_samples)\n",
        "\n",
        "# display the samples, encodings and decoded values!\n",
        "display_results(conv_output_samples, encoded, predicted, enc_shape=(7,7))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
